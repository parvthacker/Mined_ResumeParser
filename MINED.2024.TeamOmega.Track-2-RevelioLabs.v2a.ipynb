{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9acf2f66",
   "metadata": {},
   "source": [
    "<h2> MINeD Hackathon 2024 </h2>\n",
    "\n",
    "<h4> <b> Track 2 Revelio Labs </b> <br>\n",
    "<b> Track Name: </b> PeopleMetrics: Shaping the Next-Gen Workforce </h4>\n",
    "\n",
    "<b> Team Name: </b> Omega <br>\n",
    "<b> Team Members:</b> <br>\n",
    "&nbsp; Parv Thacker (Leader) <br>\n",
    "&nbsp; Richa Yadav <br>\n",
    "&nbsp; Khushi Patel <br>\n",
    "&nbsp; Niti Patel <br>\n",
    "&nbsp; Jaival Chhag <br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf2ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "from PyPDF2 import PdfReader\n",
    "import matplotlib.pyplot as plt\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")       ## To Remove warnings for rare symbols that are missing from certain fonts (for saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d956212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_docx(file_path):\n",
    "    # Extract text from DOCX file\n",
    "    text = docx2txt.process(file_path)\n",
    "    return text\n",
    "\n",
    "def extract_from_pdf(file_path):\n",
    "    # Extract text from PDF file\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        for pageNo in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[pageNo]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def create_resume_dict(text):\n",
    "    # Create a dictionary of resume sections based on headings\n",
    "    headings = ['Introduction', 'Technical skills', 'Experience']  # Add more headings as needed\n",
    "    resume_dict = {}\n",
    "    for heading in headings:\n",
    "        if heading.lower() in text.lower():\n",
    "            \n",
    "            start_index = text.lower().index(heading.lower())\n",
    "            if headings.index(heading) == len(headings) - 1:\n",
    "                end_index = len(text)\n",
    "            else:\n",
    "                next_heading_index = text.lower().find(headings[headings.index(heading) + 1].lower(), start_index)\n",
    "                end_index = next_heading_index if next_heading_index != -1 else len(text)\n",
    "                \n",
    "            section_text = text[start_index:end_index]\n",
    "            resume_dict[heading] = section_text\n",
    "    return resume_dict\n",
    "\n",
    "def extract_contact_info(text):\n",
    "    # Extract name, email, phone number, and LinkedIn URL using regular expressions\n",
    "    \n",
    "    name_match = re.search(r'[A-Za-z]+ [A-Za-z]+', text)\n",
    "    email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    phone_number_match = re.search(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})', text)\n",
    "    linkedin_match = re.search(r'https?://www\\.linkedin\\.com/[\\w\\d-]+', text)\n",
    "    \n",
    "    name = name_match.group(0) if name_match else \"Not mentioned\"\n",
    "    email = email_match.group(0) if email_match else \"Not mentioned\"\n",
    "    phone_number = phone_number_match.group(0) if phone_number_match else \"Not mentioned\"\n",
    "    linkedin_url = linkedin_match.group(0) if linkedin_match else \"Not mentioned\"\n",
    "    \n",
    "    return name, email, phone_number, linkedin_url\n",
    "\n",
    "def shortlist_resumes(resumes, shortlisting_words):\n",
    "    # Match terms in resume to identify ones that match the requirements (Match any one)\n",
    "    shortlisted_resumes = []\n",
    "    for i, resume in enumerate(resumes):\n",
    "        resume_shortlisted = False\n",
    "        for section_heading, section_text in resume.items():\n",
    "            if isinstance(section_text, list):\n",
    "                continue\n",
    "            for word in shortlisting_words:\n",
    "                if word.lower() in section_text.lower():\n",
    "                    resume_shortlisted = True\n",
    "                    break\n",
    "            if resume_shortlisted:\n",
    "                break\n",
    "        if resume_shortlisted:\n",
    "            shortlisted_resumes.append(resume)\n",
    "            print(\"Shortlisted:\", i)\n",
    "\n",
    "    return shortlisted_resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860be594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_words(main_string, word_list):\n",
    "    main_string = main_string.lower()                   # for case-insensitive matching\n",
    "    word_list = [word.lower() for word in word_list]    # convert word list to lowercase\n",
    "\n",
    "    # Split the main string into words\n",
    "    main_words = main_string.split()\n",
    "\n",
    "    # Empty list to store matching words\n",
    "    matching_words = []\n",
    "\n",
    "    # Exact matches with words from the list\n",
    "    for word in main_words:\n",
    "        if word in word_list:\n",
    "            matching_words.append(word)\n",
    "\n",
    "    # Print the matching words\n",
    "    matches = []\n",
    "    for match in matching_words:\n",
    "        matches += [match]\n",
    "        \n",
    "    return list(set(matches))\n",
    "\n",
    "# List of words to match\n",
    "word_list = [\n",
    "    \"Programming\", \"Algorithms\", \"Data Structures\", \"Databases\", \"web development\",\n",
    "    \"Networking\", \"Cybersecurity\", \"Cloud Computing\", \"Machine Learning\", \"Operating Systems\",\n",
    "    \"Problem Solving\", \"Communication\", \"Collaboration\", \"Adaptability\", \"Creativity\",\n",
    "    \"Critical Thinking\", \"Debugging\", \"Automation\", \"Version Control\", \"Analysis\",\n",
    "    \"Automation\", \"DevOps\", \"Scripting\", \"Design\", \"Optimization\", \"Trouble shooting\",\n",
    "    \"Scalability\", \"Performance\", \"Documentation\", \"Integration\", \"Testing\", \"Security\",\n",
    "    \"Agility\", \"Persistence\", \"Leadership\", \"Innovation\", \"Research\", \"Modularity\",\n",
    "    \"Efficiency\", \"Resilience\", \"Visualization\", \"Abstraction\", \"Parallelism\",\n",
    "    \"Multithreading\", \"Parallelism\", \"Time Management\", \"Interdisciplinary\", \"Analytical\",\n",
    "    \"UX/UI\", \"Versioning\", \"Debugging\", \"Flexibility\", \"Creativity\", \"Adaptability\",\n",
    "    \"Precision\", \"Initiative\", \"Resourcefulness\", \"Empathy\", \"Collaboration\", \"Determination\",\n",
    "    \"Communication\", \"Organization\", \"Leadership\", \"Attention to detail\", \"Problem solving\",\n",
    "    \"Time Management\", \"Critical Thinking\", \"Adaptability\", \"Resilience\", \"Decision Making\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d917ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_title_mapping(csv_file):\n",
    "    title_mapping = {}\n",
    "    with open(csv_file, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            reported_title = row['Reported Job Title']\n",
    "            standardized_title = row['Title']\n",
    "            onet_soc_code = row['O*NET-SOC Code']\n",
    "            title_mapping[reported_title] = {'title': standardized_title, 'code': onet_soc_code}\n",
    "    return title_mapping\n",
    "\n",
    "# Default location of the csv file\n",
    "csv_file = 'onet.csv'               # Update the locations of ONET here\n",
    "title_mapping = create_title_mapping(csv_file)\n",
    "\n",
    "# Function to standardize job titles using the mapping\n",
    "def standardize_job_titles(text, title_mapping):\n",
    "    standardized_titles = []\n",
    "    # Sort reported job titles by length in descending order\n",
    "    sorted_titles = sorted(title_mapping.keys(), key=len, reverse=True)\n",
    "    for reported_title in sorted_titles:\n",
    "        if reported_title.lower() in text.lower():\n",
    "            # If the reported title is not part of a longer reported title already included in standardized_titles\n",
    "            if not any(reported_title.lower() in title[0].lower() for title in standardized_titles):\n",
    "                standardized_title = title_mapping[reported_title]['title']\n",
    "                onet_soc_code = title_mapping[reported_title]['code']\n",
    "                standardized_titles.append([reported_title, standardized_title, onet_soc_code])\n",
    "                # Remove shorter variations of the matched reported job title from the text\n",
    "                text = text.replace(reported_title.lower(), '', 1)\n",
    "    return standardized_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e742439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to string to store it directly in unstructured/semistructured formats for easy viewing\n",
    "def convert_to_string(lst):\n",
    "    if isinstance(lst, list):\n",
    "        if all(isinstance(item, list) for item in lst):\n",
    "            return '\\n'.join(', '.join(sub) for sub in lst)\n",
    "        else:\n",
    "            return ', '.join(lst)\n",
    "    else:\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84e4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample shortlisting words\n",
    "shortlisting_words = ['Data Science', 'Data Scientist']         # Set a selection criteria, if any here\n",
    "\n",
    "# Sample resumes (file paths)\n",
    "resume_files = []\n",
    "resume_files = os.listdir(\"./resumes/\")\n",
    "for i, file in enumerate(resume_files):\n",
    "    resume_files[i] = \"./resumes/\"+file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0178e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop for explaination\n",
    "resume_files = resume_files[:5]     # The [:5 ] is added to speed up the execution for the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c895f827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortlisted: 1\n",
      "Shortlisted: 3\n",
      "Shortlisted: 4\n",
      "Shortlisted Resume 1 saved as shortlisted_resume_documents20220826-1-v01cla.pdf, shortlisted_resume_documents20220826-1-v01cla.png, and shortlisted_resume_documents20220826-1-v01cla.txt\n",
      "Shortlisted Resume 2 saved as shortlisted_resume_documents20220830-1-eq72bl.pdf, shortlisted_resume_documents20220830-1-eq72bl.png, and shortlisted_resume_documents20220830-1-eq72bl.txt\n",
      "Shortlisted Resume 3 saved as shortlisted_resume_DS Updated 2.pdf, shortlisted_resume_DS Updated 2.png, and shortlisted_resume_DS Updated 2.txt\n"
     ]
    }
   ],
   "source": [
    "# Extract text from resumes\n",
    "resumes = []\n",
    "for file in resume_files:\n",
    "    if file.endswith('.docx'):\n",
    "        text = extract_from_docx(file)\n",
    "    elif file.endswith('.pdf'):\n",
    "        text = extract_from_pdf(file)\n",
    "        \n",
    "    # Call the match_words function with the main string and word list\n",
    "    skillsex = match_words(text, word_list)\n",
    "    # print(skillsex)\n",
    "        \n",
    "    resume_dict = create_resume_dict(text)\n",
    "    name, email, phone_number, linkedin_url = extract_contact_info(text)\n",
    "    \n",
    "    resume_dict[\"FileNm\"] = file[10:-4]\n",
    "    resume_dict[\"Name\"] = name\n",
    "    resume_dict[\"Email\"] = email\n",
    "    resume_dict[\"Phone\"] = phone_number\n",
    "    resume_dict[\"LinkedIn\"] = linkedin_url\n",
    "    resume_dict[\"Skills_Extracted\"] = skillsex\n",
    "    resume_dict[\"standardized_titles\"] = standardize_job_titles(text, title_mapping)\n",
    "    resumes.append(resume_dict)\n",
    "\n",
    "\n",
    "# Shortlist resumes based on shortlisting words\n",
    "shortlisted_resumes = shortlist_resumes(resumes, shortlisting_words)\n",
    "\n",
    "# Save shortlisted resumes as images and PDFs\n",
    "for idx, resume in enumerate(shortlisted_resumes[:5], 1):\n",
    "    # Create a new PDF for each shortlisted resume\n",
    "    pdf_file_path = f\"shortlisted_resume_{resume['FileNm']}.pdf\"\n",
    "    c = canvas.Canvas(pdf_file_path, pagesize=letter)\n",
    "    \n",
    "    # Write contact information at the top\n",
    "    c.drawString(100, 770, f\"Name: {resume['Name']}\")\n",
    "    c.drawString(100, 750, f\"Email: {resume['Email']}\")\n",
    "    c.drawString(100, 730, f\"Phone: {resume['Phone']}\")\n",
    "    if resume['LinkedIn']:\n",
    "        c.linkURL(resume['LinkedIn'], f\"LinkedIn: {resume['LinkedIn']}\")\n",
    "    \n",
    "    # Write resume content to the PDF\n",
    "    y_coordinate = 710\n",
    "    for heading, section_text in resume.items():\n",
    "        section_text = convert_to_string(section_text)\n",
    "        if heading not in [\"Name\", \"Email\", \"Phone\", \"LinkedIn\"]:\n",
    "            c.drawString(100, y_coordinate, heading)\n",
    "            c.drawString(100, y_coordinate - 20, section_text)\n",
    "            y_coordinate -= 40\n",
    "    \n",
    "    c.save()\n",
    "        \n",
    "    # Save resume as image\n",
    "    image_file_path = f\"shortlisted_resume_{resume['FileNm']}.png\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.text(0.5, 0.5, \"\\n\".join([f\"{heading}:\\n{convert_to_string(section_text)}\\n\" for heading, section_text in resume.items()]), ha='center', va='center', wrap=True)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(image_file_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save resume as text file\n",
    "    txt_file_path = f\"shortlisted_resume_{resume['FileNm']}.txt\"\n",
    "    with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(\"\\n\".join([f\"{heading}:\\n{convert_to_string(section_text)}\\n\" for heading, section_text in resume.items()]))\n",
    "    \n",
    "    print(f\"Shortlisted Resume {idx} saved as {pdf_file_path}, {image_file_path}, and {txt_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016ca48",
   "metadata": {},
   "source": [
    "Sortlisted resumes are saved in the files in various formats. The data extracted is shown. ONET standardization and skills are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba8b202b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Experience': \"EXPERIENCE\\nProject Lead\\n• Spearheaded a team of five in creating a supply chain tool to address challenges across 96 counties in Indiana, empowering customers to access regional \\nproduct information. \\n• Leveraged ETL processes to assess and verify the accuracy of company data, maintaining a well-organized MySQL database. \\n• Implemented web scraping techniques with ParseHub to streamline the data gathering phase of the ETL process, reducing data collection time by 50%. \\n• Provided mentorship and guidance to junior team members, leading to a 25% increase in team productivity and a significant improvement in individual skill \\nsets over the course of the project. \\n• Collaboratively defined project goals and established strategic plans with the team, ensuring a clear vision and direction that led to successful project \\ncompletion and improved overall team performance.\\nData Scientist\\n• Project in Collaboration with Bayer and Purdue University. \\n• Handled missing data in genetic marker datasets by implementing zero imputation, enhancing the accuracy and reliability of subsequent predictive analysis \\nfor phenotypic traits. \\n• Implemented a data pipeline that divided imputed marker datasets by population generating insights and analysis on per population basis. \\n• Constructed a new SQL database on cluster 2, integrating both imputed marker and phenotypic data. This Standardized data repository supported efficient \\ndata retrieval and analysis. \\n• Developed Mixed Linear model to predict yield utilizing features such as Genetic markers and environmental features achieving an r-squared value of 0.2.\\nData Analyst\\n• Project in Collaboration with Purdue and Wabash National.\\n• Conducted univariate exploratory data analysis on telematics data for Wabash trucks across the United States. \\n• Established geofences around 70+ distribution centers for geospatial analysis, monitoring inventory levels over three years to drive informed business \\ndecisions. \\n• Optimized parallel processing pipelines using Dask and Python for handling large dataset of size over 100 GB, achieving an 85% reduction in storage levels. \\n• Communicated key findings to stakeholders using Data Visualization techniques such as Tableau, Matplotlib and Seaborn and influenced key business \\ndecisions by optimizing inventory with a projected annual profit of five percent. Documented Project metrics, results, business objectives and outcomes.\\nPROJECTS\\nAmazon Sentiment Analysis (NLP, Unstructured Data)\\n• Developed and Executed a Bidirectional GRU neural network model to conduct sentiment analysis on a robust dataset of Amazon product reviews to \\ninvestigate consumer feedback to bring business impact and recommend solutions to improve systems. \\n• Streamlined data preprocessing by leveraging FastText for effective text encoding of review data, leading to an impressive classification accuracy of 90%, \\ndemonstrating the model's high performance and reliability in sentiment prediction.\\nObject Detection and Localization (Computer Vision)\\n• Crafted a ResNET architecture boasting 98 layers and 150 million parameters, featuring cross entropy and IOU loss functions to enable precise object detection \\nand localization with an impressive 90% accuracy rate. \\n• Solved Vanishing Gradient problem by employing Skip Blocks.\\nParkinson’s Disease Classification (Predictive Modeling, Structured Data)\\n• Tackled class imbalance issues by implementing Borderline SMOTE and utilized classification algorithms like Random Forests and SVM, resulting in high \\nprecision and recall of 0.93. \\n• Minimized false-negative rates to near-zero levels, significantly enhancing prediction quality and achieved a classification accuracy of 98 percent. \\nSKILLS\\nProgramming: C++, Arduino, Python, R \\nLibraries: PyTorch, TensorFlow, Keras, Scikit-Learn, Statsmodels, Scipy, Pandas, Dask, Numpy. \\nDatabase: Postgres SQL, MySQL. \\nBI Tools: Tableau. \\nVisualization: Matplotlib, Seaborn, Geopy, Geopandas, Plotly, Folium. \\nCloud: AWS \\nAlgorithms: Regression, Classification, Clustering, Deep Learning and Time Series. \\nMathematical Core: Statistics and Probability, Linear Algebra, Multivariate Calculus, Optimization.Dauch Center for the Management of Manufacturing Enterprises West Lafayette, United States. , August 2022 - May 2023\\nBayer West Lafayette, United States. , August 2022 - December 2022\\nWabash National West Lafayette, Indiana , July 2022 - August 2022\",\n",
       " 'FileNm': 'DS Updated 2',\n",
       " 'Name': 'ASachin Asokan',\n",
       " 'Email': 'asokansachin73@gmail.com',\n",
       " 'Phone': '7654071650',\n",
       " 'LinkedIn': 'Not mentioned',\n",
       " 'Skills_Extracted': ['analysis',\n",
       "  'visualization',\n",
       "  'precision',\n",
       "  'performance',\n",
       "  'collaboration',\n",
       "  'algorithms'],\n",
       " 'standardized_titles': [['Industrial Engineer',\n",
       "   'Industrial Engineers',\n",
       "   '17-2112.00'],\n",
       "  ['Mechanical Engineer', 'Mechanical Engineers', '17-2141.00'],\n",
       "  ['Data Analyst', 'Statistical Assistants', '43-9111.00'],\n",
       "  ['Scientist', 'Geographers', '19-3092.00'],\n",
       "  ['Architect', 'Architectural Drafters', '17-3011.01'],\n",
       "  ['Model', 'Models', '41-9012.00']]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortlisted_resumes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed37469",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc26417",
   "metadata": {},
   "outputs": [],
   "source": [
    "Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
